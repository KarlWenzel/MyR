---
title: "Hierarchical Clustering"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis)
is organizing things that are similar to each other into groups.
The definition of "similar" is determined by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. 

[Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering),
also known as **hierarchical cluster analysis**, or **HCA** is a method of cluster analysis
which seeks to build a hierarchy of clusters using one of two general approaches:

- **Agglomerative**: This is a "bottom up" approach: 
each observation starts in its own cluster,
and pairs of clusters are merged as one moves up the hierarchy.
- **Divisive**: This is a "top down" approach: 
all observations start in one cluster,
and splits are performed recursively as one moves down the hierarchy.

In general, the merges and splits are determined in a
[greedy](https://en.wikipedia.org/wiki/Greedy_algorithm) manner
(in other words, it makes locally optimal choices at each stage, in a short-sighted way).
The results of hierarchical clustering are usually presented in a
[dendrogram](https://en.wikipedia.org/wiki/Dendrogram),
i.e. a tree diagram used to illustrate the hierarchical arrangment of clusters,
and may be overlayed with [heatmaps](https://en.wikipedia.org/wiki/Heat_map).

The following R functions for hierarchical clustering are covered herein:

- `dist()`
- `hclust()`
- `cutree()`
- `heatmap()`

```{r example}
set.seed(1234)

# First make some random data that has a pattern
PTS = data.frame(
    x = rnorm( 12, mean=rep(1:3, each=4), sd=0.2 ),
    y = rnorm( 12, mean=rep(c(1,2,1), each=4), sd=0.2 )
)

# Show the points...
plot(PTS$x, PTS$y, xlim=c(0,3.1), pch=19, main="Points (i.e. Vectors)", xlab="x", ylab="y")

# ...and label each point with its corresponding row number in the data frame
text(PTS$x, PTS$y, labels=1:nrow(PTS), cex=.7, pos=4)

# dist() is our distance method, and it determines Euclidean distance of the vectors
#   i.e. sqrt( (a1-a2)^2 + (b1-b2)^2 + ... + (z1-z2)^2 )
D <- dist(PTS)
print( round(D, digit=2) )

# hclust() turns the distance matrix into an hclust object
HC = hclust(D)

# Note that rows (i.e. vectors) are the domain, and their hierarchical grouping is the range
plot(HC, xlab="Row Index", ylab="Hierarchical Grouping")

# Cutting the tree into k number of branches provides grouping at a specified level
CUT = cutree(HC, k=3)
print(CUT)

# Re-plot our 2D point vectors, but color code based on cluster identifier
plot(PTS$x, PTS$y, xlim=c(0,3.1), pch=19, col=CUT, main="Points Colored by Group", xlab="x", ylab="y")
text(PTS$x, PTS$y, labels=1:nrow(PTS), cex=.7, pos=4)

# heatmap() requires a matrix.  Notice that the dimensions are the domain now. 
heatmap( as.matrix(PTS), xlab="Dimensions", ylab="Rows" )

# Using a dendrogram + heatmap is more useful for multivariate data.  
# Create a 40 x 10 matrix full of random data.
BIGM = matrix(rnorm(400), nrow=40)
head( round(BIGM, digits=2) )

# This is too random, so introduce a pattern. 
pattern = rep(c(0,3), each=5)
str(pattern)

for (i in 1:40) {
  coinFlip <- rbinom(1, size=1, prob=0.5)
  if (coinFlip) {
    BIGM[i,] = BIGM[i,] + pattern
  }
}

# Now use hierarchical cluster analysis to find the pattern.
# Notice that the dimensions are the domain, and the order of the dimensions is altered
heatmap( BIGM, xlab="Dimensions", ylab="Rows" ) 



